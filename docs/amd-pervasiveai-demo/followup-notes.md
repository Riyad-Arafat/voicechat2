# RDNA3 Unsloth Compatibility
>> Like, another bug I had, I was trying to see if I could get xformers working, but no one from AMD has respond or looked at it: https://github.com/ROCm/xformers/issues/9 - in theory vLLM had a patch for xformers support a while ago, but it ... didn't seem to work. The main thing is that w/o xformers, DeepSpeed (another thing that'd be great to get working) and (for dev/low resource use, maybe more important) Unsloth https://github.com/unslothai/unsloth won't work either - I don't know how familiar you are w/ Unsloth/Daniel https://x.com/danielhanchen (he's awesome and super enthusiastic) - there were a few of us trying to poke around in the Unsloth Discord a while ago to see if we could get it working w/ AMD cards but ran into xformers and Triton issues. (if you're looking for something high impact in the community, helping to get first class Unsloth support might be something to consider!)
>
> This is still a blocker for Unsloth AFAIK. Like I mentioned, I'm active on the Unsloth Discord so happy to make any connections if necessary, although Mike and Daniel (Unsloth team) are now YC-funded and in SF, so I assume someone's reached out to coordinate with them for AMD compatibility? They're very easy to talk to. I'm sure if they had some cards/they'd be able to squeeze a lot more out of AMD cards (like I mentioned, I recent training writeup that the Weights and Biases team featured their editorial blog w/ the W7900 in the mix https://wandb.ai/augmxnt/train-bench/reports/Trainer-performance-comparison-torchtune-vs-axolotl-vs-Unsloth---Vmlldzo4MzU3NTAx - it did OK, but on a FLOPS basis, it should be doing a lot better!)

# Faster SRT
> On faster SRT, here's the CTranslate2 issue/discussion: https://github.com/OpenNMT/CTranslate2/issues/1072 - CTranslate2 is oneDNN based, which apparently added AMD GPU support, so not an impossible lift I think. Actually looks like someone forked and did a pass last month: https://github.com/OpenNMT/CTranslate2/compare/master...arlo-phoenix:CTranslate2-rocm:rocm - arlo-phoenix is a name I recognize, he previously had a bitsandbytes fork. https://github.com/SYSTRAN/faster-whisper depends on CTranslate2 and https://github.com/m-bain/whisperX leverages FasterWhisper.
>
> I have some performance benchmarking here: https://github.com/AUGMXNT/speed-benchmarking/tree/main and some raw numbers here: https://docs.google.com/spreadsheets/d/1kT4or6b0Fedd-W_jMwYpb63e1ZR3aePczz3zlbJW-Y4/edit?pli=1&gid=1652827441#gid=1652827441 - when running HF pipe (regular whisper) you can see AMD is somewhat competitive, but basically FasterWhisper is like 5X+ faster and so the RDNA3 cards just get left in the dust, so maybe worth following up on.

# Faster LLM
> For LLM inferencing, it sounds like llama.cpp optimization is on your radar already, I think the most interesting thing to followup on there is on the APU side w/ UMA: https://github.com/ggerganov/llama.cpp/issues/7399 and exploring why the recent `-fa` implementation makes AMD GPUs slower. (Also, Nvidia perf recently pulled away by a huge amount w/ CUDA Graph support added - maybe some hipGraph work could improve things - gfx1100 has more FLOPS and MBW than an RTX 3090 but is 50-100% slower atm, so there's definitely room to optimize).

# TTS
> TTS is the biggest perf bottleneck for how voicechat2 currently runs on AMD. Sadly, basically every TTS I've tried (xTTS, StyleTTS2, Melo, etc) all runs significantly slower on RDNA3 than on the Nvidia side. I believe almost all these engines are just straight PyTorch, not doing anything fancy, but I don't have much more to add/haven't dug into this part as extensively.

# NPU
> On the NPU question, one of the people on my ShisaAI Discord (I run one of the most active English-language Japanese-focused LLM/AI communities there) did some work w/ it (I've only poked w/ the iGPU which can run the regular ROCm stack with some hacking), but I think the main issue is the RyzenAI/Xilinx Vitis stack has been totally separate and in a lot of flux. It's sort of a fractal version of the problem where some software works for Instinct but not RDNA3, some stuff works for gfx1100 but not other RDNA, etc. I get that it's tough to get different hardware families working w/ the same API and for backwards compatibility btw, but Nvidia has done it w/ CUDA and Intel is doing it w/ oneAPI, and it seems like ROCm could get there (and needs to for AMD to get better community developer traction?). It would be great to be able to dev on W7900 and know that I can deploy that same code on NPU, or RDNA2, but more importantly on say MI300X. Right now, that feels a lot further away than on the CUDA side.

# One more thing
> Maybe having a central Github Project/Discord that is actively triaged w/ the community by devrel (like what Intel has done for their Arc game compatibility here: https://github.com/IGCIT/Intel-GPU-Community-Issue-Tracker-IGCIT) would be good, that way you guys could work w/ the community to track down/prioritize which specific libs or bugs are the blockers and it would be great to have a central dashboard/repo (a table on the README page split up into software packages, hardware compatibility, etc would be great) where you could point people to in terms of what works/doesn't (since there's a lot of FUD, but also the developments are too much for any single person to track, especially when things break. Eg, when I was setting up torchtune, I ran into a recent PyTorch/hipblaslt issue that broke the world/was a bit of a doozy: https://github.com/pytorch/torchtune/discussions/1108). Anyway, something like this would make community coordination a lot easier I think, otherwise there are too many projects/repos and issues in those individual repos usually go unresolved/unlinked.
